\documentclass{article}

\usepackage{amsmath}
\usepackage{comment}
\usepackage{color}
\usepackage{graphicx}
\usepackage{enumitem}
\graphicspath{ {figure/} }

\usepackage[paper=letterpaper,margin=1in]{geometry}
\newgeometry{top=1in,bottom=1in,right=0.8in,left=0.8in}

\title{\vspace{-2.0cm}Linear Regression and Decision Trees}
\author{Klaas Fiete Krutein}

%%%% Toggle '\includecomment' line for inclusion/exclusion of comments (R code)
\includecomment{comment}

%%% comments for R code chunks: 'include=', 'label=', 'echo=', 'fig.keep=', 'results=' 

\begin{document}

%\SweaveOpts{concordance=TRUE}
% \SweaveOpts{concordance=TRUE}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\large{Problem 1 :}}
Here let's consider the following dataset
\begin{center}
  \begin{tabular}{|l|c|r|}
  \hline
    $X_1$ & $X_2$ & $Y$ \\ \hline
    -0.15 & -0.48 & 0.46 \\ \hline
    -0.72 & -0.54 & -0.37 \\ \hline
    1.36 & -0.91 & -0.27 \\ \hline
    0.61 & 1.59 & 1.35 \\ \hline
    -1.11 & 0.34 & -0.11 \\ \hline
  \end{tabular}
\end{center}
Please calculate the regression parameters ($\beta_0$,$\beta_1$, and $\beta_2$) if you are going to build a linear regression model i.e.,
$$ Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \epsilon$$

\subsubsection*{\textbf{Solution:}}
Following that:
$$\beta_0 = \bar{Y} - \beta_1\bar{X_1} - \beta_2\bar{X_2}$$
and
$$ \beta_1 = \frac{\left(\sum_{n=1}^NX_{2n}^2\right)\left(\sum_{n=1}^NX_{1n}Y\right) - \left(\sum_{n=1}^NX_{1n}X_{2n}\right)\left(\sum_{n=1}^NX_{2n}Y\right)}{\left(\sum_{n=1}^NX_{1n}^2\right)\left(\sum_{n=1}^NX_{2n}^2\right)- \left(\sum_{n=1}^NX_{1n}X_{2n}\right)^2}$$
and 
$$ \beta_2 = \frac{\left(\sum_{n=1}^NX_{1n}^2\right)\left(\sum_{n=1}^NX_{2n}Y\right) - \left(\sum_{n=1}^NX_{1n}X_{2n}\right)\left(\sum_{n=1}^NX_{1n}Y\right)}{\left(\sum_{n=1}^NX_{1n}^2\right)\left(\sum_{n=1}^NX_{2n}^2\right)- \left(\sum_{n=1}^NX_{1n}X_{2n}\right)^2}$$
We used R to calculate the values manually:
<<>>=
x_1 <- c(-0.15, -0.72, 1.36, 0.61, -1.11)
x_2 <- c(-0.48, -0.54, -0.91, 1.59, 0.34)
y <- c(0.46, -0.37, -0.27, 1.35, -0.11)
data <- cbind(x_1, x_2, y)
data <- as.data.frame(data)
# square or multiply
x_1_sq <- x_1^2 
x_2_sq <- x_2^2
x_1_x_2 <- x_1 * x_2
x_1_y <- x_1 * y
x_2_y <- x_2 * y
# now sum them
sum_x_1_sq <- sum(x_1_sq)
sum_x_2_sq <- sum(x_1_sq)
sum_x_1_x_2 <- sum(x_1_x_2)
sum_x_1_y <- sum(x_1_y)
sum_x_2_y <- sum(x_2_y)
# now calculate
beta_1 <- (sum_x_2_sq*sum_x_1_y-sum_x_1_x_2*sum_x_2_y)/(sum_x_1_sq*sum_x_2_sq-(sum_x_1_x_2)^2)
beta_2 <- (sum_x_1_sq*sum_x_2_y-sum_x_1_x_2*sum_x_1_y)/(sum_x_1_sq*sum_x_2_sq-(sum_x_1_x_2)^2)
# now we take the arithmetic average of each variable
x_1bar <- mean(x_1)
x_2bar <- mean(x_2)
ybar <- mean(y)
# now calculate
beta_0 <- ybar - beta_1 * x_1bar - beta_1 * x_2bar
# now display the equations
@
\begin{align*}
\beta_1 & = \frac{\Sexpr{sum_x_2_sq}*\Sexpr{sum_x_1_y}-(\Sexpr{sum_x_1_x_2})*\Sexpr{sum_x_2_y}}{\Sexpr{sum_x_1_sq}*\Sexpr{sum_x_2_sq}-(\Sexpr{sum_x_1_x_2})^2}\\
& = \Sexpr{beta_1}\\
\beta_2 & = \frac{\Sexpr{sum_x_1_sq}*\Sexpr{sum_x_2_y}-(\Sexpr{sum_x_1_x_2})*\Sexpr{sum_x_1_y}}{\Sexpr{sum_x_1_sq}*\Sexpr{sum_x_2_sq}-(\Sexpr{sum_x_1_x_2})^2}\\
& = \Sexpr{beta_2}\\
\beta_0 & = \Sexpr{ybar} - \Sexpr{beta_1}(\Sexpr{x_1bar}) - \Sexpr{beta_2}(\Sexpr{x_2bar})\\
& = \Sexpr{beta_0}\\
\end{align*}
Which results in the model:
$$ Y = \Sexpr{beta_0}+\Sexpr{beta_1}X_1 + \Sexpr{beta_2}X_2 + \epsilon$$ 
$$ Y = \Sexpr{round(beta_0,4)}+\Sexpr{round(beta_1,4)}X_1 + \Sexpr{round(beta_2,4)}X_2 + \epsilon$$
\pagebreak
\subsection*{\large{Problem 2:}}
Use the essential R pipeline for linear regression on this data. Compare the result from R and the result by your manual calculation. 
\subsubsection*{\textbf{Solution:}}
<<>>=
model1 <- lm(y~x_1+x_2, data=data)
summary(model1)
@

<<>>=
library(ggfortify)
require("ggfortify")
autoplot(model1, which = 1:6, ncol = 3, label.size = 3)
@
This shows that: 
$$ Y = 0.2124 + 0.2222X_1 + 0.5946X_2 + \epsilon$$
Which is almost the same model as the one we calculated manually above.The slight deviations are probably due to numerical approximation and roundings. 
\pagebreak
\subsection*{\large{Problem 3:}}
Consider the following dataset:

\begin{center}
  \begin{tabular}{|l|c|c|r|}
  \hline
    ID & $X_1$ & $X_2$ & $Y$ \\ \hline
    1 & 0.22 & 0.38 & No \\ \hline
    2 & 0.58 & 0.32 & Yes \\ \hline
    3 & 0.57 & 0.28 & Yes \\ \hline
    4 & 0.41 & 0.43 & Yes \\ \hline
    5 & 0.6 & 0.29 & No \\ \hline
    6 & 0.12 & 0.32 & Yes \\ \hline
    7 & 0.25 & 0.32 & Yes \\ \hline
    8 & 0.32 & 0.38 & No \\ 
    \hline
  \end{tabular}
\end{center}
Build a decision tree model based on the following dataset. Don't use R. Use your pen and paper and show the process.

\subsubsection*{\textbf{Solution:}}
The solution to Problem 3 will be submitted separately by Mason. 
\pagebreak
\subsection*{\large{Problem 4:}}
Use the essential R pipeline for building a decision tree model on this data. Compare the result from R and the result by your manual calculation.

\subsubsection*{\textbf{Solution:}}
<<>>=
y <- c("No", "Yes", "Yes", "Yes", "No", "Yes", "Yes", "No")
y <- as.factor(y)
dataset <- cbind(c(0.22, 0.58, 0.57, 0.41, 0.6, 0.12, 0.25, 0.32), 
                 c(0.38, 0.32, 0.28, 0.43, 0.29, 0.32, 0.32, 0.38), y)
dataset <- data.frame(dataset)
colnames(dataset)=c("x1","x2", "y")
dataset$y <- as.factor(dataset$y) # create factored dataset with no = 1, yes = 2
# create the tree
library(rpart)
library(rpart.plot)
# tree <- rpart(y ~ ., data = dataset)
tree <- rpart(y ~ ., data = dataset, control =rpart.control(minbucket = 1))
# draw the tree
prp(tree,nn.cex=1)
@
We can see that the decision tree created by R includes an aditional level but leads to finite branches earlier than the tree I have created in Problem 3.
\end{document}