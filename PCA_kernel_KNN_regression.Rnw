\documentclass{article}

\usepackage{amsmath}
\usepackage{comment}
\usepackage{color}
\usepackage{graphicx}
\usepackage{enumitem}
\graphicspath{ {figure/} }

\newcommand{\mb}{\mathbf}

\usepackage[paper=letterpaper,margin=1in]{geometry}
\newgeometry{top=1in,bottom=1in,right=0.8in,left=0.8in}

\title{\vspace{-2.0cm}PCA, Kernel and KNN Regression}
\author{Klaas Fiete Krutein}

%%%% Toggle '\includecomment' line for inclusion/exclusion of comments (R code)
\includecomment{comment}

%%% comments for R code chunks: 'include=', 'label=', 'echo=', 'fig.keep=', 'results=' 

\begin{document}

%\SweaveOpts{concordance=TRUE}
% \SweaveOpts{concordance=TRUE}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\large{Problem 1:}}
Consider the dataset in Q2 from HW4. 
\begin{enumerate}
\item Conduct the PCA analysis on the three predictors to identify the three principal components and their contributions on explaining the variance in data.
\subsubsection*{Solution:}
We are using the following dataset:
\begin{center}
  \begin{tabular}{|c|c|c|c|c|}
  \hline
    ID & $X_1$ & $X_2$ & $X_3$ & $Y$ \\ \hline
    1 & 4 & 1 & 1 & 1 \\ \hline
    2 & 4 & -1 & 0 & 1 \\ \hline
    3 & 8 & 2 & 1 & 1 \\ \hline
    4 & -2.5 & 0 & 0 & -1 \\ \hline
    5 & 0 & 1 & 1 & -1 \\ \hline
    6 & -0.3 & -1 & 0 & -1 \\ \hline
    7 & 2.5 & -1 & 1 & -1 \\ \hline
    8 & -1 & 1 & 0 & -1 \\ \hline
    \hline
  \end{tabular}
\end{center}
According to the PCA steps, we need to do the following:
\begin{enumerate}
\item Standardize $X$
To standardize $X$, we have to calculate the summary statistics of every predictor. We use R to go through the manual steps:
<<>>=
data <- as.data.frame(matrix(c(4,1,1,1,4,-1,0,1,8,2,1,
                               1,-2.5, 0,0,-1,0,1,1,-1,
                               -0.3,-1,0,-1,2.5,-1,1,-1,
                               -1,1,0,-1), nrow=8, ncol=4, byrow=TRUE))
colnames(data) <- c("x1", "x2", "x3", "y")
summary(data[,1:3])
sd(data$x1)
sd(data$x2)
sd(data$x3)
@
Let $\eta$ denote the standardization of $X$, then we can standardize $X$ through:
$$ \eta(X_i) = \frac{X_i-\mu}{\sigma}$$
<<>>=
X <- data[,1:3]
X$x1 <- (X$x1-1.837)/sd(X$x1)
X$x2 <- (X$x2-0.25)/sd(X$x2)
X$x3 <- (X$x3-0.5)/sd(X$x3)
X
@
\item Calculate $S$ through $S=\frac{\mb{X}^T\mb{X}}{n-1}$
Now we can take this $X$-matrix and calculate $S$:
<<>>=
S <- (t(as.matrix(X)) %*% as.matrix(X))/7
S <- cov(X)
S
@
\item Perform the eigen decomposition of $S$
We decompose:
<<>>=
Eigen <- eigen(S)
Eigen
@
From this we receive our weight matrices as:
\begin{align*}
\mb{w_1} & = \begin{bmatrix} 0.5764906 \\ 0.5268149 \\ 0.6245996\\ \end{bmatrix}\\
\mb{w_2} & = \begin{bmatrix} 0.5820827 \\ -0.8012385 \\ 0.1385515\\ \end{bmatrix}\\
\mb{w_3} & = \begin{bmatrix} 0.5734443 \\ 0.2836950 \\ -0.7685563\\ \end{bmatrix}\\
\end{align*}
and our $\lambda$ values as:
\begin{align*}
\lambda_1 & = 1.900506\\
\lambda_2 & = 0.6839791\\
\lambda_3 & = 0.4155140\\
\end{align*}

\item Translate $X$ to $Z$
We translate using the formula:
$$ PC_i = \mb{X}_i^T\mb{W}_i$$
<<>>=
PC <- X
colnames(PC) <- c("PC1", "PC2", "PC3")
X <- as.matrix(X)
for (i in 1:8){
  for (j in 1:3){
    PC[i,j] <- X[i,] %*% Eigen$vectors[,j]
  }
}
PC
@
\item Build a regression model of $Y$ on $Z$ that explains the variance in the dataset:
<<>>=
exp_frame <- matrix(NA, nrow=3, ncol=1)
for (i in 1:3){
  exp_frame[i,] <- Eigen$values[i]/sum(Eigen$values)
}
exp_frame
@
We can see that principal component 1 is explaining the data set to approx. 63.3\%, principal compoinent 2 to approx. 22.8\% and prinicpal component 3 to approx. 13.9\%.

\end{enumerate}
\item Use the essential R pipeline to do the PCA analysis and compare with your manual calculation.  
\subsubsection*{Solution:}
<<>>=
norm_X <- cbind(scale(data$x1), scale(data$x2), scale(data$x3))
model_pca <- eigen(cov(norm_X))
X_pc <- data.frame(norm_X %*% model_pca$vectors)
colnames(X_pc) <- c("PC1", "PC2", "PC3")
model_pca
X_pc
@
As we can see the $\lambda$ values are the same and therefore the PCs explain the same amount of the data set as in the manual calculations above.
\end{enumerate}
\subsection*{\large{Problem 2:}}
Consider the following dataset. 
\begin{center}
Training data (6 data points)\par
  \begin{tabular}{|c|c|c|}
  \hline
    ID & $X$ & $Y$ \\ \hline
    1 & -0.32 & 0.66 \\ \hline
    2 & -0.1 & 0.82 \\ \hline
    3 & 0.74 & -0.37\\ \hline
    4 & 1.21 & -0.8 \\ \hline
    5 & 0.44 & 0.52\\ \hline
    6 & -0.68 & 0.97 \\ \hline
    \hline
  \end{tabular}
\end{center}
Use the gausskernel() function from the R package “KRLS” to calculate the similarity between the data points (including the 6 training data points and the 3 testing data points in the Table below)
\subsubsection*{Solution:}
<<>>=
x <- c(-0.32, -0.1, 0.74, 1.21, 0.44, -0.68)
y <- c(0.66, 0.82, -0.37, -0.8, 0.52, 0.97)
test <- c(-1,0,1)

library(KRLS)
Kernel_m <- gausskernel(X = c(x, test), sigma=1)
Kernel_m[,7:9]
@
\subsection*{\large{Problem 3:}}
Build a kernel regression model with Gaussian kernel with bandwidth parameter, and predict on the following data points
\begin{center}
Testing data (3 data points)\par
  \begin{tabular}{|c|c|c|}
  \hline
    ID & $X$ & $Y$ \\ \hline
    7 & -1 &  \\ \hline
    8 & 0 &  \\ \hline
    9 & 1 & \\ \hline
    \hline
  \end{tabular}
\end{center}
\subsubsection*{Solution:}
We can start predicting with the following formulas:
\begin{align*}
K(x_i, x_j) & = e^{-y||x_i - x_j||^2}\\
W(x_n, x^*) & = \frac{K(x_n, x^*)}{\sum_{n=1}^N K(x_n, x^*)}\\
Y^* & = \sum_{n=1}^N y_n w(x_n, x^*)\\
y^* & = \frac{\sum_{n=1}^Ny_n [K(x_n, x^*)]}{\sum_{n=1}^N K(x_n, x^*)}\\
\end{align*}
With this we approximate $y_7^*$:
\begin{center}
  \begin{tabular}{|c|c|c|c|c|}
  \hline
    ID & $X$ & $Y$ & $K(x_n, x^*)$ & $Y_n[K(x_n, x^*)]$\\ \hline
    1 & -0.32 & 0.66 & 0.62977038 & 0.41565\\ \hline
    2 & -0.1 & 0.82 & 0.44485807 & 0.3648\\ \hline
    3 & 0.74 & -0.37 & 0.04843173 & -0.0179\\ \hline
    4 & 1.21 & -0.8 & 0.00756593 & -0.00605\\ \hline
    5 & 0.44 & 0.52 & 0.12573233 & 0.06538\\ \hline
    6 & -0.68 & 0.97 & 0.90266841 & 0.8756\\ \hline
    7 & -1 & 0.7860022 & 2.159 & 1.697\\ \hline
    \hline
  \end{tabular}
\end{center}
\begin{align*}
\sum_{n=1}^6 K(x_n, x_7^*) & = 0.62977038 + 0.44485807 + 0.04843173 + 0.00756593 + 0.12573233 + 0.90266841\\
& = 2.159027\\
\sum_{n=1}^6 y_n[K(x_n, x_7^*)] & = 0.41565 + 0.3648 -0.0179 - 0.00605 + 0.06538 + 0.8756\\
& = 1.697\\
y_7^* & = \frac{1.697}{2.159027}\\
& = 0.7860022\\
\end{align*}
We continue with $y_8^*$:
\begin{center}
  \begin{tabular}{|c|c|c|c|c|}
  \hline
    ID & $X$ & $Y$ & $K(x_n, x^*)$ & $Y_n[K(x_n, x^*)]$\\ \hline
    1 & -0.32 & 0.66 & 0.9026684 & 0.5958\\ \hline
    2 & -0.1 & 0.82 & 0.9900498 & 0.8118\\ \hline
    3 & 0.74 & -0.37 & 0.5783362 & -0.214\\ \hline
    4 & 1.21 & -0.8 & 0.2312861 & -0.185\\ \hline
    5 & 0.44 & 0.52 & 0.8239874 & 0.4285\\ \hline
    6 & -0.68 & 0.97 & 0.3678794 & 0.611\\ \hline
    \vdots & \vdots & \vdots & \vdots & \vdots\\ \hline
    8 & 0 & 0.492794 & 4.156098 & 2.0481\\ \hline
    \hline
  \end{tabular}
\end{center}
\begin{align*}
\sum_{n=1}^6 K(x_n, x_8^*) & = 0.9026684 + 0.9900498 +0.5783362 + 0.2312861 + 0.8239874 + 0.6297704\\
& = 4.156098\\
\sum_{n=1}^6 y_n[K(x_n, x_7^*)] & = 0.5958 + 0.8118 -0.214 - 0.185 + 0.4285 + 0.611\\
& = 2.0481\\
y_7^* & = \frac{2.0481}{4.156098}\\
& = 0.492794\\
\end{align*}

And finally with $y_9^*$:
\begin{center}
  \begin{tabular}{|c|c|c|c|c|}
  \hline
    ID & $X$ & $Y$ & $K(x_n, x^*)$ & $Y_n[K(x_n, x^*)]$\\ \hline
    1 & -0.32 & 0.66 & 0.17509966 & 0.1155\\ \hline
    2 & -0.1 & 0.82 & 0.29819728 & 0.2445\\ \hline
    3 & 0.74 & -0.37 & 0.93463425 & -0.3458\\ \hline
    4 & 1.21 & -0.8 & 0.95685827 & -0.7655\\ \hline
    5 & 0.44 & 0.52 & 0.73081129 & 0.38\\ \hline
    6 & -0.68 & 0.97 & 0.05946306 & 0.0577\\ \hline
    \vdots & \vdots & \vdots & \vdots & \vdots\\ \hline
    9 & 0 & -0.09939577 & 3.155064 & -0.3136\\ \hline
    \hline
  \end{tabular}
\end{center}
\begin{align*}
\sum_{n=1}^6 K(x_n, x_9^*) & = 0.17509966 + 0.29819728 +0.93463425 + 0.95685827 + 0.73081129 + 0.05946306\\
& = 3.155064\\
\sum_{n=1}^6 y_n[K(x_n, x_7^*)] & = 0.1155 + 0.2445 -0.3458 - 0.7655 + 0.38 + 0.0577\\
& = -0.3136\\
y_7^* & = \frac{-0.3136}{3.155064}\\
& = -0.09939577\\
\end{align*}

\subsection*{\large{Problem 4:}}
Use the above data set. Build a KNN regression model with K = 2. Predict on the 3 data points in the testing data.
\subsubsection*{Solution:}
We start with $K=2$, given the formulas:
\begin{align*}
y^* & = \frac{1}{K}\sum_{n=1}^N y_n\\
& = \frac{1}{K}(y_{n_{k_1}} + y_{n_{k_2}})\\
\end{align*}
\begin{align*}
y_7^* & = \frac{1}{2}(0.97 + 0.66)\\
& = 0.815\\
y_8^* & = \frac{1}{2}(0.82 + 0.66)\\
& = 0.74\\
y_9^* & = \frac{1}{2}(-0.37 -0.8)\\
& = -0.585\\
\end{align*}
And hence:
\begin{center}
  \begin{tabular}{|c|c|c|}
  \hline
    ID & $X$ & $Y$ \\ \hline
    1 & -0.32 & 0.66 \\ \hline
    2 & -0.1 & 0.82 \\ \hline
    3 & 0.74 & -0.37\\ \hline
    4 & 1.21 & -0.8 \\ \hline
    5 & 0.44 & 0.52\\ \hline
    6 & -0.68 & 0.97 \\ \hline
    7 & -1 & 0.815 \\ \hline
    8 & 0 & 0.74 \\ \hline
    9 & 1 & -0.585 \\ \hline
    \hline
  \end{tabular}
\end{center}

\subsection*{\large{Problem 5:}}
Use the essential R pipeline for KNN regression on this data. Compare the result from R and the result by your manual calculation. 
\subsubsection*{Solution:}
<<>>=
train <- data.frame(x,y)
library(FNN)
KNN_m <- knn.reg(train = train$x, test = data.frame(test), y = train$y, k = 2)
KNN_m
@
As we can see the results of the R pipeline are equal to the results of the manual calculation.



\end{document}