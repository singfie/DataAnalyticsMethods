\documentclass{article}

\usepackage{amsmath}
\usepackage{comment}
\usepackage{color}
\usepackage{graphicx}
\usepackage{enumitem}
\graphicspath{ {figure/} }

\newcommand{\mb}{\mathbf}

\usepackage[paper=letterpaper,margin=1in]{geometry}
\newgeometry{top=1in,bottom=1in,right=0.8in,left=0.8in}

\title{\vspace{-2.0cm}Weighted Least Squares and Logistic Regression}
\author{Klaas Fiete Krutein}

%%%% Toggle '\includecomment' line for inclusion/exclusion of comments (R code)
\includecomment{comment}

%%% comments for R code chunks: 'include=', 'label=', 'echo=', 'fig.keep=', 'results=' 

\begin{document}

%\SweaveOpts{concordance=TRUE}
% \SweaveOpts{concordance=TRUE}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\large{Problem 1 :}}
 Consider the case that, in building linear regression models, there is a concern that some data points may be more important (or more trustable). Thus, it is not uncommon to assign a weight to each data point. Denote the weight for the ith data point as $w_i$. An example is shown in the data table below, as the last column, e.g., $w_1=1$, $w_2=2$,$w_5=3$ . We still want to estimate the regression parameters in the least squares framework. Follow the process of the derivation of the least squares estimator as shown in the book, and propose your new estimator of the regression parameters. (hint,  put weight in the squared of the vertical derivations)
 
\subsection*{Solution:}

We use the least squares estimation from the multivariate regression model. The definition of the least squares vertical derivations for all observed data points is:
$$ \min_{\beta}\left(\mb{y-X}\beta\right)^T\left(\mb{y-X}\beta\right)$$
Before we can derive the estimator for $\beta$, we need to add the weighting through:
$$ \min_{\beta}\left(\mb{y-X}\beta\right)^T\mb{W}\left(\mb{y-X}\beta\right)$$
To solve this optimization problem, we differentiate over $\beta$ and set it equal to zero:
\begin{align*}
 \frac{\partial\left(\mb{y-X}\beta\right)^T\mb{W}\left(\mb{y-X}\beta\right)}{\partial\beta} & = 0\\
\mb{X}^T\mb{W}\left(\mb{y-X}\beta\right) & = 0\\
\mb{X}^T\mb{W}\mb{y} - \mb{X}^T\mb{WX}\beta & = 0\\
\end{align*}
And following that, we get the following least square estimator for $\hat{\beta}$:
$$ \hat{\beta} = \left(\mb{X}^T\mb{WX}\right)^{-1}\mb{X}^T\mb{W}\mb{y}$$
Where $\mb{W}$ is a diagonal matrix with the weight values on the diagonal axis.  
\pagebreak
\subsection*{\large{Problem 2:}}
Following the weighted least squares derived in Q1, please calculate the regression parameters ($\beta_0$, $\beta_1$ and $\beta_2$) if you are going to build a linear regression model using the data shown in below, i.e.,
$$ Y=\beta_0 + \beta_1X_1 + \beta_2X_2 + \epsilon $$

\begin{center}
  \begin{tabular}{|c|c|c|c|}
  \hline
    $X_1$ & $X_2$ & $Y$ & $w$\\ \hline
    -0.15 & -0.48 & 0.46 & 1 \\ \hline
    -0.72 & -0.54 & -0.37 & 2\\ \hline
    1.36 & -0.91 & -0.27 & 2\\ \hline
    0.61 & 1.59 & 1.35 & 1\\ \hline
    -1.11 & 0.34 & -0.11 & 3\\ \hline
  \end{tabular}
\end{center}

\subsection*{Solution:}
<<>>=
X <- as.matrix(cbind(1,c(-0.15, -0.72, 1.36, 0.61, -1.11),
                     c(-0.48, -0.54, -0.91, 1.59, 0.34)))
y <- as.matrix(c(0.46, -0.37, -0.27, 1.35, -0.11))
W <- as.matrix(cbind(c(1,0,0,0,0),c(0,2,0,0,0),c(0,0,2,0,0),c(0,0,0,1,0),c(0,0,0,0,3)))
@
Following:
$$ \hat{\beta} = \left(\mb{X}^T\mb{WX}\right)^{-1}\mb{X}^T\mb{W}\mb{y}$$
We can calculate:
<<>>=
beta_hat <- solve(t(X)%*%W%*%X)%*%t(X)%*%W%*%y
beta_hat
@
Thus, we get for our prediction parameter $\mb{\hat{\beta}}$:
$$ \begin{bmatrix}\hat{\beta_0}\\\hat{\beta_1}\\\hat{\beta_2}\end{bmatrix} = \begin{bmatrix} \Sexpr{beta_hat[1,]}\\\Sexpr{beta_hat[2,]} \\ \Sexpr{beta_hat[3,]} \end{bmatrix}$$
\pagebreak
\subsection*{\large{Problem 3:}}
Use the essential R pipeline for linear regression on this data (set up the weights in the lm() function). Compare the result from R and the result by your manual calculation. 

\subsection*{Solution:}
We build a linear model with weights as:
<<>>=
# reinstate the same data as a dataframe
data <- data.frame(cbind(c(-0.15, -0.72, 1.36, 0.61, -1.11),
                         c(-0.48, -0.54, -0.91, 1.59, 0.34),
                         c(0.46, -0.37, -0.27, 1.35, -0.11)))
colnames(data) <- c("X1", "X2", "Y")
weights <- c(1,2,2,1,3)
                   
linear.full <- lm(Y ~ X1 + X2, data=data, weights=weights)
summary(linear.full)
@
As the summary of the linear model shows, the coefficients are the same as for the manual calculation that resulted from Problem 2. 
We find the corresponding diagnostics plot as:
<<>>=
library(ggfortify)
require("ggfortify")
autoplot(linear.full, which = 1:6, ncol = 3, label.size = 3)
@
\pagebreak
\subsection*{\large{Problem 4:}}
Consider the following dataset:

\begin{center}
  \begin{tabular}{|c|c|c|c|}
  \hline
    ID & $X_1$ & $X_2$ & $Y$ \\ \hline
    1 & 0.22 & 0.38 & No \\ \hline
    2 & 0.58 & 0.32 & Yes \\ \hline
    3 & 0.57 & 0.28 & Yes \\ \hline
    4 & 0.41 & 0.43 & Yes \\ \hline
    5 & 0.6 & 0.29 & No \\ \hline
    6 & 0.12 & 0.32 & Yes \\ \hline
    7 & 0.25 & 0.32 & Yes \\ \hline
    8 & 0.32 & 0.38 & No \\ 
    \hline
  \end{tabular}
\end{center}

Use the essential R pipeline for building logistic regression model on this data. Compare it with the decision tree model you have built in HW1. Which one is better? 

\subsection*{Solution:}
<<>>=
dataset <- data.frame(x1 = c(0.22, 0.58, 0.57, 0.41, 0.6, 0.12, 0.25, 0.32),
                     x2 = c(0.38, 0.32, 0.28, 0.43, 0.29, 0.32, 0.32, 0.38),
                     y = c("No", "Yes", "Yes", "Yes", "No", "Yes", "Yes", "No"))
# Ensure accurate data set structure
dataset$y <- as.factor(dataset$y)
# Build a logistic regression model
logit.full <- glm(y ~ ., data = dataset, family = "binomial")
summary(logit.full)
@

<<>>=
# Display the model
library(ggfortify)
require("ggfortify")
autoplot(logit.full, which = 1:6, ncol = 3, label.size = 3)
@

\noindent When we assess the diagnostic plots of this model, we can see that the fit of this model is not ideal. Especially when looking at the "Residual vs Fitted" and the "Normal Q-Q" plot, we can see that there are clearly separable groups and the model fit does not clearly satisfy the ordinary least square (OLS) assumption. Thus, in this case, the decision tree from the last homework appears to be a better option. 
\end{document}