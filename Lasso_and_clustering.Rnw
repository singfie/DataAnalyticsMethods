\documentclass{article}

\usepackage{amsmath}
\usepackage{comment}
\usepackage{color}
\usepackage{graphicx}
\usepackage{enumitem}
\graphicspath{ {figure/} }

\newcommand{\mb}{\mathbf}

\usepackage[paper=letterpaper,margin=1in]{geometry}
\newgeometry{top=1in,bottom=1in,right=0.8in,left=0.8in}

\title{\vspace{-2.0cm}Lasso and Clustering}
\author{Klaas Fiete Krutein}

%%%% Toggle '\includecomment' line for inclusion/exclusion of comments (R code)
\includecomment{comment}

%%% comments for R code chunks: 'include=', 'label=', 'echo=', 'fig.keep=', 'results=' 

\begin{document}

%\SweaveOpts{concordance=TRUE}
% \SweaveOpts{concordance=TRUE}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\large{Problem 1:}}
Here let’s consider the following dataset.
\begin{center}
  \begin{tabular}{|c|c|c|}
  \hline
    $X_1$ & $X_2$ & $Y$ \\ \hline
    -0.15 & -0.48 & 0.46 \\ \hline
    -0.72 & -0.54 & -0.37 \\ \hline
    1.36 & -0.91 & -0.27 \\ \hline
    0.61 & 1.59 & 1.35 \\ \hline
    -1.11 & 0.34 & -0.11 \\ \hline
    \hline
  \end{tabular}
\end{center}
Set an initial value for $\lambda=1$, $\beta_1 = 0$ and $\beta_2=1$. Implement the shooting algorithm by your manual operation. Get updated values of $\beta_1$ and $\beta_2$. Do one iteration. 
\subsection*{Solution:}
We follow the guidelines in the course material on the implementation of the shooting algorithm. Thus, the general optimization problem's objective function is:
$$L(\beta_j) = || \mb{y} - \sum_{k\neq j} \mb{X}_{(:,k)}\beta_k^{(t-1)} - \mb{X}_{(:,j)}\beta_j||_2^2 + \lambda \sum_{k\neq j} |\beta_{k}^{(t-1)}| + \lambda|\beta_j|$$
Simplified this is:
$$L(\beta_j) = || \mb{y} - \mb{X}_{(:,j)}\beta_j ||_2^2 + \lambda|\beta_j|$$
Which specifies that:
  $$\hat{\beta}_j^{(t)}=\begin{cases}
    q_j - \lambda/2, & \text{if}\ q_j - \lambda/2 > 0 \\
    q_j + \lambda/2, & \text{if}\ q_j + \lambda/2 < 0 \\
    0, & \text{if}\ \lambda \geq |2q_j|\\
\end{cases}$$
where $q_j = \mb{X}^T_{(:,j)}\left(\mb{y} - \sum_{k\neq j} \mb{X}_{(;,k)}\beta_k^{(t-1)}\right)$.
Now we can use this data set for calculating our shooting algorithm. We assume as well that the offset parameter $\beta_0$ does not need to be calculated, as the predictors are standardized around 0. With $\lambda=1$, $\beta_1 = 0$ and $\beta_2=1$, we aim at updating $\hat{\beta}_1$. Hence,
$$ \mb{y} - \mb{X}_{(;,2)}\hat{\beta}_2^{(0)} = \begin{bmatrix} 0.94 \\ 0.17 \\ 0.64 \\ -0.24 \\ -0.45 \\ \end{bmatrix}$$
Hence, $q_1 = \mb{X}^T_{(;,1)} \left(\mb{y} - \mb{X}_{(;,2)}\hat{\beta}_2^{(0)}\right) = 0.96$. Since $q_1 - \lambda/2 = 0.46 > 0$, $\hat{\beta}_1^{(1)} = q_1 - \lambda/2 = 0.46$. 
For $\hat{\beta}_2$, we do the same.
$$ \mb{y} - \mb{X}_{(;,1)}\hat{\beta}_1^{(1)} = \begin{bmatrix} 0.529 \\ -0.0388 \\ -0.8956 \\ 1.0694 \\ 0.4006 \\ \end{bmatrix}$$
Thus, we can update $q_2 = \mb{X}^T_{(;,2)} \left(\mb{y} - \mb{X}_{(;,1)}\hat{\beta}_1^{(1)}\right) = 2.41859$. Since $q_2 - \lambda/2 = 1.91859 > 0$, $\hat{\beta}_2 = 1.91859$.

\subsection*{\large{Problem 2:}}
Use the essential R pipeline for LASSO on this data. Compare the result from R and the result by your manual calculation. 
\subsection*{Solution:}
<<>>=
# read in data
data <- data.frame(x1 = c(-0.15, -0.72, 1.36, 0.61, -1.11), 
                   x2 = c(-0.48, -0.54, -0.91, 1.59, 0.34), 
                   y = c(0.46, -0.37, -0.27, 1.35, -0.11))

require(glmnet)
# Fit the lasso
fit <- glmnet(as.matrix(data[,1:2]),data[,3], family=c("gaussian"))
# print all betas
print(fit$beta)
# plot the convergence
plot(fit,label = TRUE)
# fit them to a model
cv.fit = cv.glmnet(as.matrix(data[,1:2]),data[,3])
# plot the fits
plot(cv.fit)     
# Select the minimum (optimum MSE)
cv.fit$lambda.min
# display the optimal model coefficients 
coef(cv.fit, s = "lambda.min")
@
As we can see, the LASSO model has issues finding the minimal MSE model parameters, as the data set is so small, that the MSE quality is limited and minimal if no parameters are selected for $\beta$. However, we can look at the path solution figure and can see that only the second variable really has a significant influence. 

\subsection*{\large{Problem 3:}}
Consider the following dataset that has 9 data points. Let’s do clustering with 3 clusters. The initial values are shown in the table below.
\begin{center}
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
  \hline
    ID & 1.53 & 0.57 & 2.56 & 1.22 & 4.13 & 6.03 & 0.98 & 5.21 & -0.37 \\ \hline
    Label & C1 & C3 & C1 & C2 & C2 & C2 & C1 & C2 & C3 \\ \hline
    \hline
  \end{tabular}
\end{center}
\begin{enumerate}
\item Write up the Gaussian Mixture Model (GMM) that you want to estimate.
\subsection*{Solution:}
For the mode, we can create the GMM as:
$$ Y = (w_1)(y_1) + (w_2)(y_2) + (1-w_1-w_2)(y_3) \ \text{, where} \ y_1 \sim N(\mu_1, \sigma_1)$$

\item Estimate the parameters of your GMM model
\subsection*{Solution:}
<<>>=
c1 <- c(1.53, 2.56, 0.98)
meanA <- mean(c1)
varianceA <- var(c1)
wa <- 1/3

c2 <- c(1.22, 4.13, 6.03, 5.21)
meanB <- mean(c2)
varianceB <- var(c2)
wb <- 4/9

c3 <- c(0.57, -0.37)
meanC <- mean(c3)
varianceC <- var(c3)
wc <- 2/9
@

\item Update the labels with your estimated parameters
\subsection*{Solution:}
<<>>=
results <- data.frame(NA)

ID <- c(1.53, 0.57, 2.56, 1.22, 4.13, 6.03, 0.98, 5.21, -0.37)
probabilities <- matrix(data = NA, nrow = length(ID), ncol = 3)
colnames(probabilities) <- c("c1", "c2", "c3")
# Loop for estimating the probabilities
for (i in 1:length(ID)) {
   x <- ID[i]
   Prob_X_A <- (1/sqrt(2*meanA*(varianceA^2)))*exp(-(((x-meanA)^2)/(2*(varianceA^2))))
   Prob_X_B <- (1/sqrt(2*meanB*(varianceB^2)))*exp(-(((x-meanB)^2)/(2*(varianceB^2))))
   Prob_X_C <- (1/sqrt(2*meanC*(varianceC^2)))*exp(-(((x-meanC)^2)/(2*(varianceC^2))))
  
   prob_A_X <- (Prob_X_A*wa)/((Prob_X_A*wa)+(Prob_X_B*wb)+(Prob_X_C*wc))
   prob_B_X <- (Prob_X_B*wb)/((Prob_X_A*wa)+(Prob_X_B*wb)+(Prob_X_C*wc))
   prob_C_X <- (Prob_X_C*wc)/((Prob_X_A*wa)+(Prob_X_B*wb)+(Prob_X_C*wc))
  
   probabilities[i,1] <- prob_A_X
   probabilities[i,2] <- prob_B_X
   probabilities[i,3] <- prob_C_X
  
   results[,i] <- which.max(probabilities[i,])
}
results
@
\begin{center}
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
  \hline
    ID & 1.53 & 0.57 & 2.56 & 1.22 & 4.13 & 6.03 & 0.98 & 5.21 & -0.37 \\ \hline
    Label & 1 & 3 & 1 & 1 & 2 & 2 & 3 & 2 & 3 \\ \hline
    \hline
  \end{tabular}
\end{center}

\item Estimate the parameters again
\subsection*{Solution:}
<<>>=
c1 <- c(1.53, 2.56, 1.22)
meanA <- mean(c1)
varianceA <- var(c1)
wa <- 1/3

c2 <- c(4.13, 6.03, 5.21)
meanB <- mean(c2)
varianceB <- var(c2)
wb <- 1/3

c3 <- c(0.57, 0.98, -0.37)
meanC <- mean(c3)
varianceC <- mean(c3)
wc <- 1/3

results <- data.frame(NA)

ID <- c(1.53, 0.57, 2.56, 1.22, 4.13, 6.03, 0.98, 5.21, -0.37)
probabilities <- matrix(data = NA, nrow = length(ID), ncol = 3)
colnames(probabilities) <- c("c1", "c2", "c3")
# Loop for estimating the probabilities
for (i in 1:length(ID)) {
   x <- ID[i]
   Prob_X_A <- (1/sqrt(2*meanA*(varianceA^2)))*exp(-(((x-meanA)^2)/(2*(varianceA^2))))
   Prob_X_B <- (1/sqrt(2*meanB*(varianceB^2)))*exp(-(((x-meanB)^2)/(2*(varianceB^2))))
   Prob_X_C <- (1/sqrt(2*meanC*(varianceC^2)))*exp(-(((x-meanC)^2)/(2*(varianceC^2))))
  
   prob_A_X <- (Prob_X_A*wa)/((Prob_X_A*wa)+(Prob_X_B*wb)+(Prob_X_C*wc))
   prob_B_X <- (Prob_X_B*wb)/((Prob_X_A*wa)+(Prob_X_B*wb)+(Prob_X_C*wc))
   prob_C_X <- (Prob_X_C*wc)/((Prob_X_A*wa)+(Prob_X_B*wb)+(Prob_X_C*wc))
  
   probabilities[i,1] <- prob_A_X
   probabilities[i,2] <- prob_B_X
   probabilities[i,3] <- prob_C_X
  
   results[,i] <- which.max(probabilities[i,])
}
results
@
As we can see, even with the new parameter estimation, the cluster classfication remains the same, which shows that it is the best solution.
\end{enumerate}

\subsection*{\large{Problem 4:}}
Use the essential R pipeline for clustering on this data. Compare the result from R and the result by your manual calculation. 
\subsection*{Solution:}
<<>>=
data <- c(1.53, 0.57, 2.56, 1.22, 4.13, 6.03, 0.98, 5.21, -0.37)
require(mclust)
AD.Mclust <- Mclust(data, G=3)
summary(AD.Mclust,parameters = TRUE)
AD.Mclust$classification
@
As we can see the cluster structure is the same. The labels are different but the groups are equivalent.

\subsection*{\large{Problem 5:}}
Consider the following data. Assume that two trees were built on it. Calculate the variable importance of each variable in RF. 
\begin{center}
\includegraphics{pic}
\end{center}
\begin{enumerate}
\item Calculate the Gini index of each node of both trees
\subsection*{Solution:}
\begin{center}
\includegraphics[width=160mm]{gini}
\end{center}
\item Estimate the importance scores of the three variables in these RF models
\subsection*{Solution:}
Variable importance factor for $x_1 = \frac{0.12 + 0.216}{2} = 0.168$\par 
Variable importance factor for $x_2 = \frac{0.5 + 0.44}{2} = 0.47$\par Variable importance factor for $x_3 = 0$
\end{enumerate}

\end{document}